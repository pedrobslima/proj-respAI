{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk\n",
    "#%pip install spacy\n",
    "#%pip install mljar-supervised\n",
    "#%pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# default\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# tokenização e pre-processamento\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# coisas de ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modelos de ml\n",
    "from supervised.automl import AutoML\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# métricas e explicabilidade\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                               text\n",
       "0        0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1        0  is upset that he can't update his Facebook by ...\n",
       "2        0  @Kenichan I dived many times for the ball. Man...\n",
       "3        0    my whole body feels itchy and like its on fire \n",
       "4        0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets.csv\",encoding='ISO-8859-1', names=['emotion', 'id', 'date', 'query', 'user', 'text'])\n",
    "df = df.drop(columns=['id', 'date', 'query', 'user'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset ca updat facebook text might cri result ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>dive mani time ball manag save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>behav mad ca see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                               text  \\\n",
       "0        0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        0  is upset that he can't update his Facebook by ...   \n",
       "2        0  @Kenichan I dived many times for the ball. Man...   \n",
       "3        0    my whole body feels itchy and like its on fire    \n",
       "4        0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                           text_norm  \n",
       "0       awww bummer shoulda got david carr third day  \n",
       "1  upset ca updat facebook text might cri result ...  \n",
       "2       dive mani time ball manag save rest go bound  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                                   behav mad ca see  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "def normalize_text(text):\n",
    "    global vocab\n",
    "    # Normalização de case\n",
    "    text = text.lower()\n",
    "    # Remoção de usuários e links\n",
    "    # - (tem que fazer antes da tokenização pq ele não tava tirando as menções, já q o '@' ficou como um token separado)\n",
    "    # - o .isalpha() dps do .split() iria tirar toda palavra que tivesse terminando com um acento ou vírgula, ele deve ser usado dps do word_tokenize\n",
    "    text = ' '.join([word for word in text.split() if not(('@' in word) or ('.com' in word))]) \n",
    "    # Tokenização\n",
    "    text = word_tokenize(text)\n",
    "    # Remoção de stopwords\n",
    "    text_norm = []\n",
    "    for word in text:\n",
    "        # Remoção de stopwords\n",
    "        if((word not in stop_words) and (word.isalpha())):\n",
    "            # Stemming\n",
    "            word = stemmer.stem(word)\n",
    "            # Adicionando à nova lista/texto\n",
    "            text_norm.append(word)\n",
    "            # Criação do vocabulário (p/ filtragem por freq.)\n",
    "            if(word in vocab.keys()):\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    \n",
    "    return text_norm #(ainda não juntei p/ n ter q separar td dnv na filtragem por freq.)\n",
    "\n",
    "def freq_filter(text):\n",
    "    global vocab\n",
    "    text = [word for word in text if vocab[word] > 1]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df[\"text_norm\"] = df[\"text\"].apply(normalize_text).apply(freq_filter)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 81823)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['emotion']==4, 'emotion'] = 1 # mudando a classe positiva de 4 para 1\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"text_norm\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"emotion\"], test_size=0.3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.shape:\n",
    "* antes: (1600000, 460181)\n",
    "* dps do filtro: (1600000, 169304)\n",
    "* com filtragem de user e links antes (+ filtro de freq): (1600000, 65381)\n",
    "* filtragem revisada: (1600000, 81823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               Treinamento\t|  Teste\n",
      "Acurácia:    0.77815089286\t|  0.75710833333\n",
      "Precisão[1]: 0.77776589525\t|  0.75568047460\n",
      "Precisão[0]: 0.77853733415\t|  0.75854868071\n",
      "Recall:      0.77901137372\t|  0.75944865311\n",
      "TNR:         0.77728994783\t|  0.75477095661\n",
      "F1-Score:    0.77838813627\t|  0.75755987806\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'''\n",
    "               Treinamento\\t|  Teste\n",
    "Acurácia:    {accuracy_score(y_train, model.predict(X_train)):.11f}\\t|  {accuracy_score(y_test, model.predict(X_test)):.11f}\n",
    "Precisão[1]: {precision_score(y_train, model.predict(X_train), pos_label=1):.11f}\\t|  {precision_score(y_test, model.predict(X_test), pos_label=1):.11f}\n",
    "Precisão[0]: {precision_score(y_train, model.predict(X_train), pos_label=0):.11f}\\t|  {precision_score(y_test, model.predict(X_test), pos_label=0):.11f}\n",
    "Recall:      {recall_score(y_train, model.predict(X_train), pos_label=1):.11f}\\t|  {recall_score(y_test, model.predict(X_test), pos_label=1):.11f}\n",
    "TNR:         {recall_score(y_train, model.predict(X_train), pos_label=0):.11f}\\t|  {recall_score(y_test, model.predict(X_test), pos_label=0):.11f}\n",
    "F1-Score:    {f1_score(y_train, model.predict(X_train), pos_label=1):.11f}\\t|  {f1_score(y_test, model.predict(X_test), pos_label=1):.11f}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acurácia caiu em 0.0007, não é nd dms. As métricas não melhoraram, mas pelo menos o custo computacional sim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 683. GiB for an array with shape (1120000, 81823) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m automl \u001b[38;5;241m=\u001b[39m AutoML(\n\u001b[0;32m      2\u001b[0m     total_time_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m, \n\u001b[0;32m      3\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplain\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m     ml_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_classification\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m automl\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, y_train)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ^ Se não usar o .toarray(), dá erro dizendo que precisa de dados densos;\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#   mas se usar, o .toarray() tenta alocar 3.5TiB de memória pra fazer a conversão e dá erro\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# > Update: dps da redução da quant. de tokens pelos filtros, \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#   o .toarray() ainda dá erro pq agr tenta alocar só 546GiB\u001b[39;00m\n\u001b[0;32m     13\u001b[0m automl_predict \u001b[38;5;241m=\u001b[39m automl\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\pedro\\miniconda3\\envs\\mainPy\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pedro\\miniconda3\\envs\\mainPy\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 683. GiB for an array with shape (1120000, 81823) and data type float64"
     ]
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    total_time_limit=5*60, \n",
    "    mode='Explain', \n",
    "    ml_task='binary_classification'\n",
    ")\n",
    "\n",
    "automl.fit(X_train.toarray(), y_train)\n",
    "# ^ Se não usar o .toarray(), dá erro dizendo que precisa de dados densos;\n",
    "#   mas se usar, o .toarray() tenta alocar 3.5TiB de memória pra fazer a conversão e dá erro\n",
    "# > Update: dps da redução da quant. de tokens pelos filtros, \n",
    "#   o .toarray() ainda dá erro pq agr tenta alocar só 683GiB\n",
    "\n",
    "automl_predict = automl.predict(X_test)\n",
    "\n",
    "#model = MultinomialNB()\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'Test Accuracy for AutoML: {accuracy_score(y_test, automl_predict)}')\n",
    "\n",
    "automl.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME:\n",
    "\n",
    "([tutorial p/ texto](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
